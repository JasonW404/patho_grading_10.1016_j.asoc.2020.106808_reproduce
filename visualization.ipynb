{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openslide\n",
    "from pathlib import Path\n",
    "\n",
    "# Use the first file found\n",
    "wsi_dir = Path(\"dataset/training_dataset/training_image_data\")\n",
    "svs_path = next(wsi_dir.glob(\"*.svs\"))\n",
    "\n",
    "print(f\"Inspecting: {svs_path}\")\n",
    "try:\n",
    "    slide = openslide.OpenSlide(str(svs_path))\n",
    "    print(f\"Levels: {slide.level_count}\")\n",
    "    print(f\"Dimensions: {slide.dimensions}\")\n",
    "    print(f\"Level dimensions: {slide.level_dimensions}\")\n",
    "    print(f\"Level downsamples: {slide.level_downsamples}\")\n",
    "    print(f\"Objective power: {slide.properties.get('openslide.objective-power')}\")\n",
    "    print(f\"Magnification: {slide.properties.get('aperio.AppMag')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c3190",
   "metadata": {},
   "source": [
    "# Visualization of Stage 1 Results\n",
    "\n",
    "This notebook visualizes the results of the Stage 1 preprocessing pipeline. It compares the original WSI thumbnails with the normalized images, blue ratio images, and generated masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e91c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import openslide\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eaa65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the report\n",
    "report_path = Path(\"output/stage_one/reports/stage_one_summary.csv\")\n",
    "if not report_path.exists():\n",
    "    print(f\"Report not found at {report_path}. Please run stage 1 pipeline first.\")\n",
    "else:\n",
    "    df = pd.read_csv(report_path)\n",
    "    print(f\"Loaded report with {len(df)} records.\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ede2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path: str | Path, is_svs_thumbnail: bool = False, thumb_size: tuple[int, int] = (1024, 1024)) -> Optional[np.ndarray]:\n",
    "    \"\"\"Load an image, handling SVS files by extracting a thumbnail.\"\"\"\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        print(f\"Warning: File not found: {path}\")\n",
    "        return None\n",
    "        \n",
    "    if path.suffix.lower() == '.svs':\n",
    "        try:\n",
    "            with openslide.OpenSlide(str(path)) as slide:\n",
    "                img = slide.get_thumbnail(thumb_size)\n",
    "                return np.array(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading SVS {path}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        try:\n",
    "            return np.array(Image.open(path))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {path}: {e}\")\n",
    "            return None\n",
    "\n",
    "def visualize_row(row):\n",
    "    \"\"\"Visualize a single row of the report.\"\"\"\n",
    "    orig_path = row['image_path']\n",
    "    norm_path = row['normalized_path']\n",
    "    blue_path = row['blue_ratio_path']\n",
    "    mask_path = row['mask_path']\n",
    "    \n",
    "    orig = load_image(orig_path, is_svs_thumbnail=True)\n",
    "    norm = load_image(norm_path)\n",
    "    blue = load_image(blue_path)\n",
    "    mask = load_image(mask_path)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "    \n",
    "    # Original\n",
    "    if orig is not None:\n",
    "        axes[0].imshow(orig)\n",
    "        axes[0].set_title(f\"Original (Thumbnail)\\n{Path(orig_path).name}\")\n",
    "    else:\n",
    "        axes[0].text(0.5, 0.5, \"Image Not Found\", ha='center')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Normalized\n",
    "    if norm is not None:\n",
    "        axes[1].imshow(norm)\n",
    "        axes[1].set_title(\"Macenko Normalized\")\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, \"Image Not Found\", ha='center')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Blue Ratio\n",
    "    if blue is not None:\n",
    "        axes[2].imshow(blue, cmap='jet')\n",
    "        axes[2].set_title(\"Blue Ratio\")\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, \"Image Not Found\", ha='center')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Mask\n",
    "    if mask is not None:\n",
    "        axes[3].imshow(mask, cmap='gray')\n",
    "        axes[3].set_title(f\"Stroma Mask\\nBlobs: {row['blob_count']}, Avg Area: {row['average_blob_area']:.1f}\")\n",
    "    else:\n",
    "        axes[3].text(0.5, 0.5, \"Image Not Found\", ha='center')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a4b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first 5 processed images\n",
    "if report_path.exists():\n",
    "    for i, row in df.head(5).iterrows():\n",
    "        print(f\"Visualizing Index {i}\")\n",
    "        visualize_row(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa3d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this notebook cell to verify the code logic before editing the main file.\n",
    "# Simulating the split logic for _train_det_paper\n",
    "\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MockRow:\n",
    "    case_id: str\n",
    "    label: int\n",
    "\n",
    "def split_rows(rows, seed):\n",
    "    # Unique cases\n",
    "    case_ids = sorted(list(set(r.case_id for r in rows)))\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(case_ids)\n",
    "    \n",
    "    n = len(case_ids)\n",
    "    n_train = int(0.6 * n)\n",
    "    n_val = int(0.2 * n)\n",
    "    \n",
    "    train_cases = set(case_ids[:n_train])\n",
    "    val_cases = set(case_ids[n_train:n_train+n_val])\n",
    "    test_cases = set(case_ids[n_train+n_val:])\n",
    "    \n",
    "    train_rows = [r for r in rows if r.case_id in train_cases]\n",
    "    val_rows = [r for r in rows if r.case_id in val_cases]\n",
    "    test_rows = [r for r in rows if r.case_id in test_cases]\n",
    "    \n",
    "    return train_rows, val_rows, test_rows\n",
    "\n",
    "# Mock data\n",
    "rows = [MockRow(f\"case_{i%10}\", i%2) for i in range(100)] \n",
    "# 10 cases, 10 rows each.\n",
    "tr, va, te = split_rows(rows, 1337)\n",
    "print(f\"Total rows: {len(rows)}\")\n",
    "print(f\"Train rows: {len(tr)}\")\n",
    "print(f\"Val rows: {len(va)}\")\n",
    "print(f\"Test rows: {len(te)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bc531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will write the new function content to a string to verify syntax before editing.\n",
    "code_to_insert = \"\"\"\n",
    "@torch.no_grad()\n",
    "def _eval_det_loader(\n",
    "    *,\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    max_batches: int = 0,\n",
    ") -> dict[str, float]:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "\n",
    "    loss_sum = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    tp = fp = fn = tn = 0\n",
    "    kept = 0\n",
    "    \n",
    "    all_y_true = []\n",
    "    all_y_score = []\n",
    "\n",
    "    for images, labels in loader:\n",
    "        if max_batches > 0 and kept >= max_batches:\n",
    "            break\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        images = _imagenet_normalize_batch(images)\n",
    "\n",
    "        out = model(images)\n",
    "        loss = criterion(out, labels)\n",
    "        loss_sum += float(loss.detach().cpu())\n",
    "\n",
    "        # For AUC (class 1 prob)\n",
    "        probs = F.softmax(out, dim=1)[:, 1]\n",
    "        all_y_score.append(probs.detach().cpu().numpy())\n",
    "        all_y_true.append(labels.detach().cpu().numpy())\n",
    "\n",
    "        preds = torch.argmax(out, dim=1)\n",
    "        correct += int((preds == labels).sum().cpu())\n",
    "        total += int(labels.numel())\n",
    "        \n",
    "        pred_pos = preds == 1\n",
    "        gt_pos = labels == 1\n",
    "        tp += int((pred_pos & gt_pos).sum().cpu())\n",
    "        fp += int((pred_pos & (~gt_pos)).sum().cpu())\n",
    "        fn += int(((~pred_pos) & gt_pos).sum().cpu())\n",
    "        tn += int(((~pred_pos) & (~gt_pos)).sum().cpu())\n",
    "\n",
    "        kept += 1\n",
    "\n",
    "    avg_loss = float(loss_sum / max(1, kept))\n",
    "    acc = float(correct / max(1, total))\n",
    "    precision = float(tp / max(1, tp + fp))\n",
    "    recall = float(tp / max(1, tp + fn))\n",
    "    f1 = float((2 * precision * recall) / max(1e-12, precision + recall))\n",
    "    \n",
    "    auc = 0.0\n",
    "    if all_y_true:\n",
    "        y_true_flat = np.concatenate(all_y_true)\n",
    "        y_score_flat = np.concatenate(all_y_score)\n",
    "        if len(np.unique(y_true_flat)) > 1:\n",
    "            auc = float(roc_auc_score(y_true_flat, y_score_flat))\n",
    "\n",
    "    model.train()\n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"acc\": acc,\n",
    "        \"auc\": auc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"tp\": float(tp),\n",
    "        \"fp\": float(fp),\n",
    "        \"fn\": float(fn),\n",
    "        \"tn\": float(tn),\n",
    "    }\n",
    "\n",
    "\n",
    "def _train_det_paper(\n",
    "    *,\n",
    "    index_csv: Path,\n",
    "    checkpoint_path: Path,\n",
    "    metrics_csv: Path,\n",
    "    batch_size: int,\n",
    "    tune_epochs: int,\n",
    "    final_epochs: int,\n",
    "    device: str,\n",
    "    split_seed: int,\n",
    "    early_stop_patience: int = 5,\n",
    ") -> None:\n",
    "    \\\"\\\"\\\"Paper-aligned CNN_det training: 60/20/20 case split.\n",
    "\n",
    "    - Reads 80x80 candidate patch index.\n",
    "    - Splits by case_id.\n",
    "    - Tuning phase: Train on 60%, Val on 20% (save best).\n",
    "    - Final phase: Train on 80% (60+20), monitor on 20% test (early stop).\n",
    "    - Model: AlexNet (pretrained), resized inputs to 227x227.\n",
    "    - Hyperparameters: SGD lr=0.0001, momentum=0.9, weight_decay=0.0005.\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    index_csv = Path(index_csv)\n",
    "    checkpoint_path = Path(checkpoint_path)\n",
    "    checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    metrics_csv = Path(metrics_csv)\n",
    "    _append_metrics_row(metrics_csv, {\"event\": \"start\", \"device\": device})\n",
    "\n",
    "    # Load and split\n",
    "    rows = read_det_patch_index(index_csv)\n",
    "    case_ids = sorted(list(set(r.case_id for r in rows if r.case_id)))\n",
    "    rng = random.Random(int(split_seed))\n",
    "    rng.shuffle(case_ids)\n",
    "\n",
    "    n_total = len(case_ids)\n",
    "    n_train = int(0.6 * n_total)\n",
    "    n_val = int(0.2 * n_total)\n",
    "    \n",
    "    train_cases = set(case_ids[:n_train])\n",
    "    val_cases = set(case_ids[n_train:n_train+n_val])\n",
    "    test_cases = set(case_ids[n_train+n_val:])\n",
    "    \n",
    "    train_rows = [r for r in rows if r.case_id in train_cases]\n",
    "    val_rows = [r for r in rows if r.case_id in val_cases]\n",
    "    test_rows = [r for r in rows if r.case_id in test_cases]\n",
    "    \n",
    "    logger.info(\n",
    "        \"CNN_det split (cases): total=%d train=%d val=%d test=%d seed=%d\",\n",
    "        n_total, len(train_cases), len(val_cases), len(test_cases), int(split_seed)\n",
    "    )\n",
    "    logger.info(\n",
    "        \"CNN_det split (patches): total=%d train=%d val=%d test=%d\",\n",
    "        len(rows), len(train_rows), len(val_rows), len(test_rows)\n",
    "    )\n",
    "\n",
    "    # config implies using config.alexnet_input_size, but we can hardcode 227 as per paper/user request\n",
    "    # or load from config. User said \"AlexNet requires 227x227\".\n",
    "    cfg = load_mfc_cnn_config()\n",
    "    \n",
    "    # Datasets with normalization=False because we normalize in loop (standard practice here)\n",
    "    train_ds = PreparedMitosisDetectionPatchDataset(rows=train_rows, output_size=227, normalize_imagenet=False)\n",
    "    val_ds = PreparedMitosisDetectionPatchDataset(rows=val_rows, output_size=227, normalize_imagenet=False)\n",
    "    test_ds = PreparedMitosisDetectionPatchDataset(rows=test_rows, output_size=227, normalize_imagenet=False)\n",
    "    \n",
    "    # Paper-aligned detection training typically shuffles training data.\n",
    "    train_loader = DataLoader(train_ds, batch_size=int(batch_size), shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=int(batch_size), shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=int(batch_size), shuffle=False, num_workers=0)\n",
    "\n",
    "    device_t = _resolve_torch_device(device)\n",
    "    model = CNNDet(num_classes=2, pretrained=True).to(device_t)\n",
    "\n",
    "    # Paper hypers: LR=0.0001, Momentum=0.9, WD=0.0005\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=0.0001,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0005,\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_score: float | None = None\n",
    "    best_state: dict[str, torch.Tensor] | None = None\n",
    "    global_step = 0\n",
    "    no_improve = 0\n",
    "\n",
    "    def _run_one_epoch(*, loader: DataLoader) -> dict[str, float]:\n",
    "        nonlocal global_step\n",
    "        running_loss = 0.0\n",
    "        steps = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        tp = fp = fn = tn = 0\n",
    "\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device_t)\n",
    "            labels = labels.to(device_t)\n",
    "            images = _imagenet_normalize_batch(images)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += float(loss.detach().cpu())\n",
    "            steps += 1\n",
    "            global_step += 1\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += int((preds == labels).sum().cpu())\n",
    "                total += int(labels.numel())\n",
    "                pred_pos = preds == 1\n",
    "                gt_pos = labels == 1\n",
    "                tp += int((pred_pos & gt_pos).sum().cpu())\n",
    "                fp += int((pred_pos & (~gt_pos)).sum().cpu())\n",
    "                fn += int(((~pred_pos) & gt_pos).sum().cpu())\n",
    "                tn += int(((~pred_pos) & (~gt_pos)).sum().cpu())\n",
    "            \n",
    "            if steps == 1 or steps % 50 == 0:\n",
    "                logger.info(\"CNN_det train step=%d loss=%.4f acc=%.3f\", steps, float(loss), float(correct/total))\n",
    "\n",
    "        avg = running_loss / max(1, steps)\n",
    "        acc = float(correct / max(1, total))\n",
    "        dice = float((2 * tp) / max(1, 2 * tp + fp + fn))\n",
    "        return {\"loss\": avg, \"acc\": acc, \"dice\": dice}\n",
    "\n",
    "    logger.info(\"Starting Tuning Phase (Train on 60%, Val on 20%)\")\n",
    "    for epoch in range(int(tune_epochs)):\n",
    "        model.train()\n",
    "        train_m = _run_one_epoch(loader=train_loader)\n",
    "        val_m = _eval_det_loader(model=model, loader=val_loader, device=device_t)\n",
    "        \n",
    "        score = val_m[\"acc\"]  # Primary metric for detection is often simple Acc or F1. Paper mentions \"monitoring\".\n",
    "        # Let's use F1 as it's more robust for imbalance. Or Acc. User prompt says \"monitoring training...\".\n",
    "        # Let's log all.\n",
    "        \n",
    "        logger.info(\n",
    "            \"CNN_det tune epoch=%d train_loss=%.4f train_acc=%.3f val_loss=%.4f val_acc=%.3f val_f1=%.3f val_auc=%.3f\",\n",
    "            epoch + 1, train_m[\"loss\"], train_m[\"acc\"], val_m[\"loss\"], val_m[\"acc\"], val_m[\"f1\"], val_m[\"auc\"]\n",
    "        )\n",
    "        \n",
    "        _append_metrics_row(metrics_csv, {\n",
    "            \"phase\": \"tune\",\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": train_m[\"loss\"],\n",
    "            \"val_loss\": val_m[\"loss\"],\n",
    "            \"val_acc\": val_m[\"acc\"],\n",
    "            \"val_auc\": val_m[\"auc\"]\n",
    "        })\n",
    "\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        logger.info(\"Restored best model from tuning phase (acc=%.3f)\", best_score)\n",
    "    \n",
    "    # Final Phase: Merge Train + Val\n",
    "    logger.info(\"Starting Final Phase (Train on 80%, Monitor on 20% Test)\")\n",
    "    \n",
    "    # Create merged dataset\n",
    "    final_rows = train_rows + val_rows\n",
    "    final_ds = PreparedMitosisDetectionPatchDataset(rows=final_rows, output_size=227, normalize_imagenet=False)\n",
    "    final_loader = DataLoader(final_ds, batch_size=int(batch_size), shuffle=True, num_workers=0)\n",
    "    \n",
    "    no_improve = 0\n",
    "    best_test_score = 0.0\n",
    "    \n",
    "    for epoch in range(int(final_epochs)):\n",
    "        model.train()\n",
    "        train_m = _run_one_epoch(loader=final_loader)\n",
    "        test_m = _eval_det_loader(model=model, loader=test_loader, device=device_t)\n",
    "        \n",
    "        score = test_m[\"acc\"] # Monitor metric\n",
    "        \n",
    "        logger.info(\n",
    "            \"CNN_det final epoch=%d train_loss=%.4f train_acc=%.3f test_loss=%.4f test_acc=%.3f test_f1=%.3f test_auc=%.3f\",\n",
    "            epoch + 1, train_m[\"loss\"], train_m[\"acc\"], test_m[\"loss\"], test_m[\"acc\"], test_m[\"f1\"], test_m[\"auc\"]\n",
    "        )\n",
    "        \n",
    "        _append_metrics_row(metrics_csv, {\n",
    "            \"phase\": \"final\",\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_loss\": train_m[\"loss\"],\n",
    "            \"test_loss\": test_m[\"loss\"],\n",
    "            \"test_acc\": test_m[\"acc\"],\n",
    "            \"test_auc\": test_m[\"auc\"]\n",
    "        })\n",
    "        \n",
    "        if score > best_test_score:\n",
    "            best_test_score = score\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            \n",
    "        if early_stop_patience > 0 and no_improve >= early_stop_patience:\n",
    "             logger.info(\"Early stopping triggered at final epoch %d\", epoch+1)\n",
    "             break\n",
    "             \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model\": \"CNNDet\", \n",
    "            \"paper_aligned\": True,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"config\": cfg.model_dump(mode=\"json\")\n",
    "        },\n",
    "        checkpoint_path\n",
    "    )\n",
    "    logger.info(\"Saved CNN_det(paper) checkpoint: %s\", str(checkpoint_path))\n",
    "\n",
    "\"\"\"\n",
    "print(\"Syntax OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0fcbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototyping _train_global_paper logic\n",
    "\n",
    "def split_global_rows_paper(rows, seed):\n",
    "    # Unique slides\n",
    "    slide_ids = sorted(list(set(r.slide_id for r in rows)))\n",
    "    # Labels for stratification? Paper says \"split WSI cases\". Usually stratified by class.\n",
    "    # We can try to stratify if we have labels per slide (we do).\n",
    "    \n",
    "    slide_to_label = {}\n",
    "    for r in rows:\n",
    "        if r.slide_id not in slide_to_label:\n",
    "            slide_to_label[r.slide_id] = r.label\n",
    "            \n",
    "    # Stratified split\n",
    "    by_label = {}\n",
    "    for s, l in slide_to_label.items():\n",
    "        by_label.setdefault(l, []).append(s)\n",
    "        \n",
    "    train_slides = set()\n",
    "    val_slides = set()\n",
    "    test_slides = set()\n",
    "    \n",
    "    rng = random.Random(seed)\n",
    "    \n",
    "    for l, slides in by_label.items():\n",
    "        rng.shuffle(slides)\n",
    "        n = len(slides)\n",
    "        n_train = int(0.6 * n)\n",
    "        n_val = int(0.2 * n)\n",
    "        \n",
    "        train_slides.update(slides[:n_train])\n",
    "        val_slides.update(slides[n_train:n_train+n_val])\n",
    "        test_slides.update(slides[n_train+n_val:])\n",
    "        \n",
    "    train_rows = [r for r in rows if r.slide_id in train_slides]\n",
    "    val_rows = [r for r in rows if r.slide_id in val_slides]\n",
    "    test_rows = [r for r in rows if r.slide_id in test_slides]\n",
    "    \n",
    "    return train_rows, val_rows, test_rows\n",
    "\n",
    "# Mock\n",
    "@dataclass\n",
    "class MockGlobalRow:\n",
    "    slide_id: str\n",
    "    label: int\n",
    "    \n",
    "grows = []\n",
    "for i in range(100):\n",
    "    slide_id = f\"S{i}\"\n",
    "    label = i % 3 + 1\n",
    "    # 10 patches per slide\n",
    "    for _ in range(10):\n",
    "        grows.append(MockGlobalRow(slide_id, label))\n",
    "        \n",
    "tr, va, te = split_global_rows_paper(grows, 1337)\n",
    "print(f\"Total: {len(grows)} (100 slides)\")\n",
    "print(f\"Train: {len(tr)}\")\n",
    "print(f\"Val: {len(va)}\")\n",
    "print(f\"Test: {len(te)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patho_grading_10.1016_j.asoc.2020.106808_reproduce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
